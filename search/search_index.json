{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"langchain-prefect","text":""},{"location":"#orchestrate-and-observe-langchain-using-prefect","title":"Orchestrate and observe langchain using Prefect","text":"<p>Large Language Models (LLMs) are interesting and useful\u200a - \u200abuilding apps that use them responsibly feels like a no-brainer. Tools like Langchain make it easier to build apps using LLMs. We need to know details about how our apps work, even when we want to use tools with convenient abstractions that may obfuscate those details.</p> <p>Prefect is built to help data people build, run, and observe event-driven workflows wherever they want. It provides a framework for creating deployments on a whole slew of runtime environments (from Lambda to Kubernetes), and is cloud agnostic (best supports AWS, GCP, Azure). For this reason, it could be a great fit for observing apps that use LLMs.</p>"},{"location":"#features","title":"Features","text":"<ul> <li><code>RecordLLMCalls</code> is a <code>ContextDecorator</code> that can be used to track LLM calls made by Langchain LLMs as Prefect flows.</li> </ul>"},{"location":"#call-an-llm-and-track-the-invocation-with-prefect","title":"Call an LLM and track the invocation with Prefect:","text":"<p><pre><code>from langchain.llms import OpenAI\nfrom langchain_prefect.plugins import RecordLLMCalls\n\nwith RecordLLMCalls():\n    llm = OpenAI(temperature=0.9)\n    text = (\n        \"What would be a good company name for a company that makes colorful socks?\"\n    )\n    llm(text)\n</code></pre> and a flow run will be created to track the invocation of the LLM:</p> <p> </p>"},{"location":"#run-several-llm-calls-via-langchain-agent-as-prefect-subflows","title":"Run several LLM calls via langchain agent as Prefect subflows:","text":"<pre><code>from langchain.agents import initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\nfrom prefect import flow\n\nllm = OpenAI(temperature=0)\ntools = load_tools([\"llm-math\"], llm=llm)\nagent = initialize_agent(tools, llm)\n\n@flow\ndef my_flow():\n    agent.run(\n        \"How old is the current Dalai Lama? \"\n        \"What is his age divided by 2 (rounded to the nearest integer)?\"\n    )\n\nwith RecordLLMCalls(tags={\"agent\"}):\n    my_flow()\n</code></pre> <p>Find more examples here.</p>"},{"location":"#how-do-i-get-a-prefect-ui","title":"How do I get a Prefect UI?","text":"<ul> <li> <p>The easiest way is to use the Prefect Cloud UI for free. You can find details on getting setup here.</p> </li> <li> <p>If you don't want to sign up for cloud, you can use the dashboard locally by running <code>prefect server start</code> in your terminal - more details here.</p> </li> </ul>"},{"location":"#resources","title":"Resources","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install langchain-prefect\n</code></pre> <p>Requires an installation of Python 3.10+.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you encounter any bugs while using <code>langchain-prefect</code>, feel free to open an issue in the langchain-prefect repository.</p> <p>Feel free to star or watch <code>langchain-prefect</code> for updates too!</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you'd like to help contribute to fix an issue or add a feature to <code>langchain-prefect</code>, please propose changes through a pull request from a fork of the repository.</p> <p>Here are the steps:</p> <ol> <li>Fork the repository</li> <li>Clone the forked repository</li> <li>Install the repository and its dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Make desired changes</li> <li>Add tests</li> <li>Insert an entry to CHANGELOG.md</li> <li>Install <code>pre-commit</code> to perform quality checks prior to commit: <pre><code>pre-commit install\n</code></pre></li> <li><code>git commit</code>, <code>git push</code>, and create a pull request</li> </ol>"},{"location":"plugins/","title":"Plugins","text":""},{"location":"plugins/#langchain_prefect.plugins","title":"<code>langchain_prefect.plugins</code>","text":"<p>Module for defining Prefect plugins for langchain.</p>"},{"location":"plugins/#langchain_prefect.plugins-classes","title":"Classes","text":""},{"location":"plugins/#langchain_prefect.plugins.RecordLLMCalls","title":"<code>RecordLLMCalls</code>","text":"<p>         Bases: <code>ContextDecorator</code></p> <p>Context decorator for patching LLM calls with a prefect flow.</p> Source code in <code>langchain_prefect/plugins.py</code> <pre><code>class RecordLLMCalls(ContextDecorator):\n\"\"\"Context decorator for patching LLM calls with a prefect flow.\"\"\"\n\n    def __init__(self, **decorator_kwargs):\n\"\"\"Context decorator for patching LLM calls with a prefect flow.\n\n        Args:\n            tags: Tags to apply to flow runs created by this context manager.\n            flow_kwargs: Keyword arguments to pass to the flow decorator.\n            max_prompt_tokens: The maximum number of tokens allowed in a prompt.\n\n        Example:\n            Create a flow with `a_custom_tag` upon calling `OpenAI.generate`:\n\n            &gt;&gt;&gt; with RecordLLMCalls(tags={\"a_custom_tag\"}):\n            &gt;&gt;&gt;    llm = OpenAI(temperature=0.9)\n            &gt;&gt;&gt;    llm(\n            &gt;&gt;&gt;        \"What would be a good company name \"\n            &gt;&gt;&gt;        \"for a company that makes carbonated water?\"\n            &gt;&gt;&gt;    )\n\n            Track many LLM calls when using a langchain agent\n\n            &gt;&gt;&gt; llm = OpenAI(temperature=0)\n            &gt;&gt;&gt; tools = load_tools([\"llm-math\"], llm=llm)\n            &gt;&gt;&gt; agent = initialize_agent(tools, llm)\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():  # noqa: D103\n            &gt;&gt;&gt;     agent.run(\n            &gt;&gt;&gt;         \"How old is the current Dalai Lama? \"\n            &gt;&gt;&gt;         \"What is his age divided by 2 (rounded to the nearest integer)?\"\n            &gt;&gt;&gt;     )\n\n            &gt;&gt;&gt; with RecordLLMCalls():\n            &gt;&gt;&gt;     my_flow()\n\n            Create an async flow upon calling `OpenAI.agenerate`:\n\n            &gt;&gt;&gt; with RecordLLMCalls():\n            &gt;&gt;&gt;    llm = OpenAI(temperature=0.9)\n            &gt;&gt;&gt;    await llm.agenerate(\n            &gt;&gt;&gt;        [\n            &gt;&gt;&gt;            \"Good name for a company that makes colorful socks?\",\n            &gt;&gt;&gt;            \"Good name for a company that sells carbonated water?\",\n            &gt;&gt;&gt;        ]\n            &gt;&gt;&gt;    )\n\n            Create flow for LLM call and enforce a max number of tokens in the prompt:\n\n            &gt;&gt;&gt; with RecordLLMCalls(max_prompt_tokens=100):\n            &gt;&gt;&gt;    llm = OpenAI(temperature=0.9)\n            &gt;&gt;&gt;    llm(\n            &gt;&gt;&gt;        \"What would be a good company name \"\n            &gt;&gt;&gt;        \"for a company that makes carbonated water?\"\n            &gt;&gt;&gt;    )\n        \"\"\"\n        self.decorator_kwargs = decorator_kwargs\n\n    def __enter__(self):\n\"\"\"Called when entering the context manager.\n\n        This is what would need to be changed if Langchain started making\n        LLM api calls in a different place.\n        \"\"\"\n        self.patched_methods = []\n        for subcls in BaseLanguageModel.__subclasses__():\n            if subcls.__name__ == \"BaseChatModel\":\n                for subsubcls in subcls.__subclasses__():\n                    # patch `BaseChatModel` generate methods when used as callable\n                    self._patch_method(subsubcls, \"_generate\", record_llm_call)\n                    self._patch_method(subsubcls, \"_agenerate\", record_llm_call)\n\n            self._patch_method(subcls, \"generate\", record_llm_call)\n            self._patch_method(subcls, \"agenerate\", record_llm_call)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n\"\"\"Reset methods when exiting the context manager.\"\"\"\n        for cls, method_name, original_method in self.patched_methods:\n            setattr(cls, method_name, original_method)\n\n    def _patch_method(self, cls, method_name, decorator):\n\"\"\"Patch a method on a class with a decorator.\"\"\"\n        original_method = getattr(cls, method_name)\n        modified_method = decorator(original_method, **self.decorator_kwargs)\n        setattr(cls, method_name, modified_method)\n        self.patched_methods.append((cls, method_name, original_method))\n</code></pre>"},{"location":"plugins/#langchain_prefect.plugins.RecordLLMCalls-functions","title":"Functions","text":""},{"location":"plugins/#langchain_prefect.plugins.RecordLLMCalls.__enter__","title":"<code>__enter__</code>","text":"<p>Called when entering the context manager.</p> <p>This is what would need to be changed if Langchain started making LLM api calls in a different place.</p> Source code in <code>langchain_prefect/plugins.py</code> <pre><code>def __enter__(self):\n\"\"\"Called when entering the context manager.\n\n    This is what would need to be changed if Langchain started making\n    LLM api calls in a different place.\n    \"\"\"\n    self.patched_methods = []\n    for subcls in BaseLanguageModel.__subclasses__():\n        if subcls.__name__ == \"BaseChatModel\":\n            for subsubcls in subcls.__subclasses__():\n                # patch `BaseChatModel` generate methods when used as callable\n                self._patch_method(subsubcls, \"_generate\", record_llm_call)\n                self._patch_method(subsubcls, \"_agenerate\", record_llm_call)\n\n        self._patch_method(subcls, \"generate\", record_llm_call)\n        self._patch_method(subcls, \"agenerate\", record_llm_call)\n</code></pre>"},{"location":"plugins/#langchain_prefect.plugins.RecordLLMCalls.__exit__","title":"<code>__exit__</code>","text":"<p>Reset methods when exiting the context manager.</p> Source code in <code>langchain_prefect/plugins.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n\"\"\"Reset methods when exiting the context manager.\"\"\"\n    for cls, method_name, original_method in self.patched_methods:\n        setattr(cls, method_name, original_method)\n</code></pre>"},{"location":"plugins/#langchain_prefect.plugins.RecordLLMCalls.__init__","title":"<code>__init__</code>","text":"<p>Context decorator for patching LLM calls with a prefect flow.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <p>Tags to apply to flow runs created by this context manager.</p> required <code>flow_kwargs</code> <p>Keyword arguments to pass to the flow decorator.</p> required <code>max_prompt_tokens</code> <p>The maximum number of tokens allowed in a prompt.</p> required Example <p>Create a flow with <code>a_custom_tag</code> upon calling <code>OpenAI.generate</code>:</p> <p>with RecordLLMCalls(tags={\"a_custom_tag\"}):    llm = OpenAI(temperature=0.9)    llm(        \"What would be a good company name \"        \"for a company that makes carbonated water?\"    )</p> <p>Track many LLM calls when using a langchain agent</p> <p>llm = OpenAI(temperature=0) tools = load_tools([\"llm-math\"], llm=llm) agent = initialize_agent(tools, llm)</p> <p>@flow def my_flow():  # noqa: D103     agent.run(         \"How old is the current Dalai Lama? \"         \"What is his age divided by 2 (rounded to the nearest integer)?\"     )</p> <p>with RecordLLMCalls():     my_flow()</p> <p>Create an async flow upon calling <code>OpenAI.agenerate</code>:</p> <p>with RecordLLMCalls():    llm = OpenAI(temperature=0.9)    await llm.agenerate(        [            \"Good name for a company that makes colorful socks?\",            \"Good name for a company that sells carbonated water?\",        ]    )</p> <p>Create flow for LLM call and enforce a max number of tokens in the prompt:</p> <p>with RecordLLMCalls(max_prompt_tokens=100):    llm = OpenAI(temperature=0.9)    llm(        \"What would be a good company name \"        \"for a company that makes carbonated water?\"    )</p> Source code in <code>langchain_prefect/plugins.py</code> <pre><code>def __init__(self, **decorator_kwargs):\n\"\"\"Context decorator for patching LLM calls with a prefect flow.\n\n    Args:\n        tags: Tags to apply to flow runs created by this context manager.\n        flow_kwargs: Keyword arguments to pass to the flow decorator.\n        max_prompt_tokens: The maximum number of tokens allowed in a prompt.\n\n    Example:\n        Create a flow with `a_custom_tag` upon calling `OpenAI.generate`:\n\n        &gt;&gt;&gt; with RecordLLMCalls(tags={\"a_custom_tag\"}):\n        &gt;&gt;&gt;    llm = OpenAI(temperature=0.9)\n        &gt;&gt;&gt;    llm(\n        &gt;&gt;&gt;        \"What would be a good company name \"\n        &gt;&gt;&gt;        \"for a company that makes carbonated water?\"\n        &gt;&gt;&gt;    )\n\n        Track many LLM calls when using a langchain agent\n\n        &gt;&gt;&gt; llm = OpenAI(temperature=0)\n        &gt;&gt;&gt; tools = load_tools([\"llm-math\"], llm=llm)\n        &gt;&gt;&gt; agent = initialize_agent(tools, llm)\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():  # noqa: D103\n        &gt;&gt;&gt;     agent.run(\n        &gt;&gt;&gt;         \"How old is the current Dalai Lama? \"\n        &gt;&gt;&gt;         \"What is his age divided by 2 (rounded to the nearest integer)?\"\n        &gt;&gt;&gt;     )\n\n        &gt;&gt;&gt; with RecordLLMCalls():\n        &gt;&gt;&gt;     my_flow()\n\n        Create an async flow upon calling `OpenAI.agenerate`:\n\n        &gt;&gt;&gt; with RecordLLMCalls():\n        &gt;&gt;&gt;    llm = OpenAI(temperature=0.9)\n        &gt;&gt;&gt;    await llm.agenerate(\n        &gt;&gt;&gt;        [\n        &gt;&gt;&gt;            \"Good name for a company that makes colorful socks?\",\n        &gt;&gt;&gt;            \"Good name for a company that sells carbonated water?\",\n        &gt;&gt;&gt;        ]\n        &gt;&gt;&gt;    )\n\n        Create flow for LLM call and enforce a max number of tokens in the prompt:\n\n        &gt;&gt;&gt; with RecordLLMCalls(max_prompt_tokens=100):\n        &gt;&gt;&gt;    llm = OpenAI(temperature=0.9)\n        &gt;&gt;&gt;    llm(\n        &gt;&gt;&gt;        \"What would be a good company name \"\n        &gt;&gt;&gt;        \"for a company that makes carbonated water?\"\n        &gt;&gt;&gt;    )\n    \"\"\"\n    self.decorator_kwargs = decorator_kwargs\n</code></pre>"},{"location":"plugins/#langchain_prefect.plugins-functions","title":"Functions","text":""},{"location":"plugins/#langchain_prefect.plugins.record_llm_call","title":"<code>record_llm_call</code>","text":"<p>Decorator for wrapping a Langchain LLM call with a prefect flow.</p> Source code in <code>langchain_prefect/plugins.py</code> <pre><code>def record_llm_call(\n    func: Callable[..., LLMResult],\n    tags: set | None = None,\n    max_prompt_tokens: int | None = int(1e4),\n    flow_kwargs: dict | None = None,\n) -&gt; Callable[..., Flow]:\n\"\"\"Decorator for wrapping a Langchain LLM call with a prefect flow.\"\"\"\n\n    tags = tags or set()\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n\"\"\"wrapper for LLM calls\"\"\"\n        invocation_artifact = llm_invocation_summary(\n            invocation_fn=func, *args, **kwargs\n        )\n\n        llm_endpoint = invocation_artifact.content[\"llm_endpoint\"]\n        prompts = invocation_artifact.content[\"prompts\"]\n\n        if max_prompt_tokens and (\n            (N := num_tokens(get_prompt_content(prompts))) &gt; max_prompt_tokens\n        ):\n            raise ValueError(\n                f\"Prompt is too long: it contains {N} tokens\"\n                f\" and {max_prompt_tokens=}. Did not call {llm_endpoint!r}. \"\n                \"If desired, increase `max_prompt_tokens`.\"\n            )\n\n        llm_generate = flow_wrapped_fn(func, flow_kwargs, *args, **kwargs)\n\n        with prefect_tags(*[llm_endpoint, *tags]):\n            return llm_generate.with_options(\n                flow_run_name=f\"Calling {llm_endpoint}\"  # noqa: E501\n            )(llm_input=invocation_artifact)\n\n    return wrapper\n</code></pre>"},{"location":"utilities/","title":"Utilities","text":""},{"location":"utilities/#langchain_prefect.utilities","title":"<code>langchain_prefect.utilities</code>","text":"<p>Utilities for the langchain_prefect package.</p>"},{"location":"utilities/#langchain_prefect.utilities-classes","title":"Classes","text":""},{"location":"utilities/#langchain_prefect.utilities.NotAnArtifact","title":"<code>NotAnArtifact</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Placeholder class for soon-to-come <code>Artifact</code>.</p> Source code in <code>langchain_prefect/utilities.py</code> <pre><code>class NotAnArtifact(BaseModel):\n\"\"\"Placeholder class for soon-to-come `Artifact`.\"\"\"\n\n    name: str\n    description: str\n    content: Any\n</code></pre>"},{"location":"utilities/#langchain_prefect.utilities-functions","title":"Functions","text":""},{"location":"utilities/#langchain_prefect.utilities.flow_wrapped_fn","title":"<code>flow_wrapped_fn</code>","text":"<p>Define a function to be wrapped in a flow depending on whether the original function is sync or async.</p> Source code in <code>langchain_prefect/utilities.py</code> <pre><code>def flow_wrapped_fn(\n    func: Callable[..., LLMResult],\n    flow_kwargs: dict | None = None,\n    *args,\n    **kwargs,\n) -&gt; Flow:\n\"\"\"Define a function to be wrapped in a flow depending\n    on whether the original function is sync or async.\"\"\"\n    flow_kwargs = flow_kwargs or dict(name=\"Execute LLM Call\", log_prints=True)\n\n    if is_async_fn(func):\n\n        async def execute_async_llm_call(llm_input: NotAnArtifact) -&gt; LLMResult:\n\"\"\"async flow for async LLM calls via `SubclassofBaseLLM.agenerate`\"\"\"\n            print(llm_input.content[\"summary\"])\n            llm_result = await func(*args, **kwargs)\n            print(f\"Recieved: {parse_llm_result(llm_result)!r}\")\n            return llm_result\n\n        return flow(**flow_kwargs)(execute_async_llm_call)\n    else:\n\n        def execute_llm_call(llm_input: NotAnArtifact) -&gt; LLMResult:\n\"\"\"sync flow for sync LLM calls via `SubclassofBaseLLM.generate`\"\"\"\n            print(llm_input.content[\"summary\"])\n            llm_result = func(*args, **kwargs)\n            print(f\"Recieved: {parse_llm_result(llm_result)!r}\")\n            return llm_result\n\n        return flow(**flow_kwargs)(execute_llm_call)\n</code></pre>"},{"location":"utilities/#langchain_prefect.utilities.get_prompt_content","title":"<code>get_prompt_content</code>","text":"<p>Return the content of the prompts.</p> Source code in <code>langchain_prefect/utilities.py</code> <pre><code>def get_prompt_content(prompts: Any) -&gt; List[str]:\n\"\"\"Return the content of the prompts.\"\"\"\n    if isinstance(prompts[0], str):\n        return prompts\n    elif isinstance(prompts[0], BaseMessage):\n        return [p.content for p in prompts]\n    else:\n        return [p.content for msg_list in prompts for p in msg_list]\n</code></pre>"},{"location":"utilities/#langchain_prefect.utilities.llm_invocation_summary","title":"<code>llm_invocation_summary</code>","text":"<p>Will eventually return an artifact.</p> Source code in <code>langchain_prefect/utilities.py</code> <pre><code>def llm_invocation_summary(*args, **kwargs) -&gt; NotAnArtifact:\n\"\"\"Will eventually return an artifact.\"\"\"\n\n    subcls, prompts, *rest = args\n\n    invocation_fn = kwargs[\"invocation_fn\"]\n\n    llm_endpoint = subcls.__module__\n\n    prompt_content = get_prompt_content(prompts)\n\n    summary = (\n        f\"Sending {listrepr([truncate(p) for p in prompt_content])} \"\n        f\"to {llm_endpoint} via {invocation_fn!r}\"\n    )\n\n    return NotAnArtifact(\n        name=\"LLM Invocation Summary\",\n        description=f\"Query {llm_endpoint} via {invocation_fn.__name__}\",\n        content={\n            \"llm_endpoint\": llm_endpoint,\n            \"prompts\": prompts,\n            \"summary\": summary,\n            \"args\": rest,\n            **kwargs,\n        },\n    )\n</code></pre>"},{"location":"utilities/#langchain_prefect.utilities.num_tokens","title":"<code>num_tokens</code>","text":"<p>Returns the number of tokens in a text string.</p> Source code in <code>langchain_prefect/utilities.py</code> <pre><code>def num_tokens(text: str | List[str], encoding_name: str = \"cl100k_base\") -&gt; int:\n\"\"\"Returns the number of tokens in a text string.\"\"\"\n    if isinstance(text, list):\n        text = \"\".join(text)\n\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(text))\n    return num_tokens\n</code></pre>"},{"location":"utilities/#langchain_prefect.utilities.parse_llm_result","title":"<code>parse_llm_result</code>","text":"<p>Will eventually return an artifact.</p> Source code in <code>langchain_prefect/utilities.py</code> <pre><code>def parse_llm_result(llm_result: LLMResult) -&gt; NotAnArtifact:\n\"\"\"Will eventually return an artifact.\"\"\"\n    return NotAnArtifact(\n        name=\"LLM Result\",\n        description=\"The result of the LLM invocation.\",\n        content=llm_result,\n    )\n</code></pre>"},{"location":"utilities/#langchain_prefect.utilities.truncate","title":"<code>truncate</code>","text":"<p>Truncate text to max_length.</p> Source code in <code>langchain_prefect/utilities.py</code> <pre><code>def truncate(text: str, max_length: int = 300) -&gt; str:\n\"\"\"Truncate text to max_length.\"\"\"\n    if len(text) &gt; 3 and len(text) &gt;= max_length:\n        i = (max_length - 3) // 2\n        return f\"{text[:i]}...{text[-i:]}\"\n    return text\n</code></pre>"}]}